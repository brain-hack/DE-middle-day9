{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.4.2.17)\n",
      "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.4.2.17)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "libxrender-dev is already the newest version (1:0.9.10-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install opencv-python==3.4.2.17\n",
    "! pip3 install opencv-contrib-python==3.4.2.17\n",
    "! pip3 install requests \n",
    "! pip3 install beautifulsoup4\n",
    "! apt install -y libsm6 libxext6 libxrender-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.6/dist-packages\r\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch)\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import http\n",
    "from bs4 import element\n",
    "import urllib\n",
    "import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "#opencv-contrib-python==3.4.2.17\n",
    "#opencv-python==3.4.2.17\n",
    "import cv2 as cv\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es,name):\n",
    "    if not es.indices.exists(name):\n",
    "        response = es.indices.create(name)\n",
    "\n",
    "        if response[\"acknowledged\"]:\n",
    "            print(\"to create index is successful : index name = '{}'\".format(response[\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data(es,index,data,doc_type,id):\n",
    "    return es.index(index=index, doc_type=doc_type, body=data,id=id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD #1: OpenCV, NumPy, and urllib\n",
    "def url_to_image(url):\n",
    "    # download the image, convert it to a NumPy array, and then read\n",
    "    # it into OpenCV format\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv.imdecode(image, cv.IMREAD_COLOR)\n",
    "\n",
    "    # return the image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_url2hash(url):\n",
    "\n",
    "    img = url_to_image(url)\n",
    "\n",
    "    gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # Initiate BRIEF extractor\n",
    "    brief = cv.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "\n",
    "    kp = sift.detect(img,None)\n",
    "\n",
    "    # compute the descriptors with BRIEF\n",
    "    kp, des = brief.compute(img, kp)\n",
    "\n",
    "\n",
    "    hash_list = []\n",
    "    for x in des:\n",
    "        hash_list.append('{}'.format(x.tobytes().hex()))\n",
    "    return hash_list\n",
    "\n",
    "\n",
    "category_map = {\n",
    "    1011010000 : \"TV/영상가전\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "himark_url = 'http://www.e-himart.co.kr'\n",
    "category_endpoint = 'http://www.e-himart.co.kr/app/display/showDisplayCategory?dispNo='\n",
    "page_counting_param = '#pageCount={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_info(title_div,product_info):\n",
    "    str_filter = re.compile('[^0-9a-zA-Zㄱ-힗\\[\\] ]')\n",
    "    title = str_filter.sub(' ', title_div.h2.text).lstrip().rstrip()\n",
    "    product_info['title'] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_promote_info(title_div,product_info):\n",
    "    #promote optional\n",
    "    if title_div.div:\n",
    "        str_filter = re.compile('[^0-9a-zA-Zㄱ-힗\\[\\] ]')\n",
    "        promote = str_filter.sub(' ', title_div.div.text).lstrip().rstrip()\n",
    "        product_info['promote'] = promote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_and_promote_info(soup, product_info):\n",
    "    title_div = soup.find(\"div\", attrs={\"class\": \"prdName\"})\n",
    "    add_title_info(title_div,product_info)\n",
    "    add_promote_info(title_div,product_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_name_info(soup,product_info):\n",
    "    if soup.find(\"div\", attrs={\"id\": \"divModelName\"}):\n",
    "        model_name = soup.find(\"div\", attrs={\"id\": \"divModelName\"}).text\n",
    "        model_name = model_name.lstrip().rstrip()\n",
    "        product_info['model_name'] = model_name\n",
    "    elif soup.find(\"span\", attrs={\"class\": \"foL\"}):\n",
    "        model_name = soup.find(\"span\", attrs={\"class\": \"foL\"}).text\n",
    "        model_name = model_name.lstrip().rstrip()\n",
    "        product_info['model_name'] = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_price_info(soup,product_info):\n",
    "    ## 가격\n",
    "    price_area = soup.find(\"li\", attrs={\"class\": \"priceArea\"}).find_all(\"span\", attrs={\"class\": \"price\"})\n",
    "    sale_price = int(price_area[0].text.replace(',',''))\n",
    "    product_info['sale_price'] = sale_price\n",
    "    advantage_price = int(price_area[1].text.replace(',',''))\n",
    "    product_info['advantage_price'] = advantage_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_star_point(soup,product_info):\n",
    "    #별점 optional\n",
    "    if soup.find(\"div\", attrs={\"class\": \"gmL\"}):\n",
    "        star_point = float(soup.find(\"div\", attrs={\"class\": \"gmL\"}).text)\n",
    "        product_info['star_point'] = star_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_img_info(soup,product_info):\n",
    "    #image\n",
    "    img_link = soup.find(\"img\",attrs={\"id\": \"imgGoodsBigImage\"})[\"src\"]\n",
    "    product_info['img_link'] = img_link\n",
    "    img_hash = img_url2hash(img_link)\n",
    "    product_info['img_hashs'] = img_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_product_info_by_soup(soup,product_id):\n",
    "    product_info = {}\n",
    "    add_title_and_promote_info(soup, product_info)\n",
    "    add_model_name_info(soup, product_info)\n",
    "    add_price_info(soup, product_info)\n",
    "    add_star_point(soup, product_info)\n",
    "    add_img_info(soup, product_info)\n",
    "    return product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_info_url(url):\n",
    "    product_id = int(url.split('=')[1])\n",
    "    req = requests.get(url)\n",
    "    if req.status_code == http.HTTPStatus.OK:\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        product_info = add_product_info_by_soup(soup,product_id)\n",
    "        product_info['product_id'] = product_id\n",
    "        return product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_each_element(es,product_list):\n",
    "    for product_item in product_list:\n",
    "        if type(product_item) == element.Tag:\n",
    "            if product_item.div:\n",
    "                if product_item.div.a:\n",
    "                    url = himark_url+product_item.div.a['href']\n",
    "                    product_info = get_product_info_url(url)\n",
    "                    \n",
    "                    if product_info:\n",
    "                        result = index_data(es,'himart',product_info,'product',product_info['product_id'])\n",
    "                        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_himart_by_page(es,url,page_num):    \n",
    "    req = requests.get(url+page_counting_param.format(page_num))\n",
    "    if req.status_code == http.HTTPStatus.OK:\n",
    "        # HTML 소스 가져오기\n",
    "        html = req.text\n",
    "        # BeautifulSoup으로 html소스를 python객체로 변환하기\n",
    "        # 첫 인자는 html소스코드, 두 번째 인자는 어떤 parser를 이용할지 명시.\n",
    "        # 이 글에서는 Python 내장 html.parser를 이용했다.\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        product_list = soup.find(\"ul\", attrs={\"id\": \"productList\"})\n",
    "        index_each_element(es,product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"localhost:9200\",http_auth=('elastic', 'changeme'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_himart_by_page(es,'http://www.e-himart.co.kr/app/display/showDisplayCategory?dispNo=1011010000',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image(url):\n",
    "    hash_list = img_url2hash(url)\n",
    "\n",
    "    hash_query = \"\"\n",
    "    for i, hash in enumerate(hash_list):\n",
    "        if i > 10:\n",
    "            hash_query += hash + \" \"\n",
    "            break;\n",
    "    query_body = {\n",
    "        'query':{\n",
    "            'match':{\n",
    "                \"img_hashs\": hash_query\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es.search(index=\"himart\", body=query_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-403ec8193e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://static2.e-himart.co.kr/contents/goods/00/01/65/04/32/0001650432__UN65NU7180FXKR__M_450_450.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2d477140be72>\u001b[0m in \u001b[0;36msearch_image\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhash_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mhash_query\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "res = search_image('http://static2.e-himart.co.kr/contents/goods/00/01/65/04/32/0001650432__UN65NU7180FXKR__M_450_450.jpg')\n",
    "print(res['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
